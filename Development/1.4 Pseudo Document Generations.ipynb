{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Document Generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spherecluster import  VonMisesFisherMixture, sample_vMF\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import  KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"word vectors.kv\"\n",
    "model = KeyedVectors.load(filename, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = np.array(model.wv.vectors)\n",
    "vocab = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing word embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfnorm = np.linalg.norm(word_embedding, axis=1, ord=2)\n",
    "word_embedding = word_embedding / linfnorm[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(word_embedding.T, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load classes and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords_str  = open('class keywords.txt', encoding='utf-8').read()\n",
    "class_keywords = {i.split(': ')[0]: i.split(': ')[1].split(', ') for i in class_keywords_str.split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain word vectors for each keyword from every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords = {topic: [i[0] for i in model.wv.most_similar (topic, topn = 100)] for topic in class_keywords.keys()}\n",
    "class_keywords_supplied = {class_label: [np.array(words_df[word]) for word in words] \n",
    "                           for class_label, words in class_keywords.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit a vMF distribution for every class, obtain cluster centers and dispersion parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vMFs = {}\n",
    "\n",
    "for i in class_keywords_supplied.keys():\n",
    "    keyword_mtx = np.vstack(class_keywords_supplied[i])\n",
    "    vmF = VonMisesFisherMixture(n_clusters=1, n_jobs=10, max_iter= 20)\n",
    "    vmF.fit(keyword_mtx)\n",
    "    topic_vMFs[i] = (vmF.cluster_centers_[0], vmF.concentrations_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build background words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.load('word_counts.npy').item()\n",
    "total_length = sum(word_counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from background word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_distributions = {i : word_count/total_length for i, word_count in word_counts.items()}\n",
    "word_distributions = pd.DataFrame.from_records(word_distributions,index=[0])\n",
    "word_distributions = word_distributions[words_df.columns]\n",
    "#np.random.choice(list(word_distributions.keys()) + , p = list(word_distributions.values()), size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordDistribution(alpha, word_distributions, top_n_keywords, words_df, topic):\n",
    "    \"\"\" Generates psuedo documents distribution given a topic\n",
    "    \n",
    "        Input: alpha - balancing parameter between background words and keywords\n",
    "               word_distributions - background words distributions\n",
    "               top_n_keywords - number n, top n class keywords\n",
    "               word_embedding - word embedding matrix\n",
    "               topic - topic keywords \n",
    "               \n",
    "        Output: a pseudo document distributions\n",
    "    \"\"\"\n",
    "    mu, kappa = topic_vMFs[topic]\n",
    "    di = sample_vMF(mu, kappa, num_samples = 1)\n",
    "    \n",
    "    di_similarities = np.exp(np.dot(di, words_df.values).ravel()) # find similarities for all words with di\n",
    "    ranked_index = np.argsort(di_similarities)[::-1] # sort the simlarity values descending, save the indices\n",
    "    \n",
    "    # set the simiarity values of all other than top n keywords to 0\n",
    "    di_similarities[ranked_index[top_n_keywords:]] = 0\n",
    "    \n",
    "    # generate document distributions\n",
    "    keywords_distributions = di_similarities/np.sum(di_similarities)\n",
    "    background_words = word_distributions.values.ravel()\n",
    "    \n",
    "    document_distributions = (alpha* np.array(background_words)\n",
    "                                      + (1 - alpha)* keywords_distributions.ravel())\n",
    "    \n",
    "#    document_distributions = pd.DataFrame(document_distributions, index = list(words_df.columns))\n",
    "#     keywords_distributions = pd.DataFrame(keywords_distributions, index = vocab)\n",
    "    \n",
    "    return document_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function that generates pseudo label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePseudoLabels(alpha, word_distributions, topic):\n",
    "    \"\"\" Generates psuedo labels given a topic\n",
    "        Input: alpha - balancing parameter between background words and keywords\n",
    "               vocab - vocabulary lists\n",
    "               topic - topic keywords\n",
    "        Output: a vector similiar to one-hot, with the largest probabilities at the topic keyword\n",
    "    \"\"\"\n",
    "    # generate pseudo label\n",
    "    background_words = word_distributions.values.ravel()\n",
    "    label_vector = np.ones(len(background_words))*alpha/len(background_words)\n",
    "    label_vector[list(word_distributions).index(topic)] += 1 - alpha\n",
    "    return label_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function that generates labelled pseudo documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLabelledPseudoDocuments(alpha, doc_length, num_docs):\n",
    "    \"\"\" Generates psuedo documents given a topic\n",
    "        Input: alpha - balancing parameter between background words and keywords\n",
    "               doc_length - length of words in the pseudo document\n",
    "               num_docs - number of documents in a batch\n",
    "        Output: a tuple (pseudo docs, pseudo labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    topics = class_keywords_supplied.keys()\n",
    "    topic_docs = {}\n",
    "    for topic in topics:\n",
    "        pseudo_docs = []\n",
    "        pseudo_labels = []\n",
    "        for i in range(num_docs):\n",
    "        \n",
    "            document_distribution = generateWordDistribution(alpha, word_distributions, \n",
    "                                                         20, words_df, topic)\n",
    "            pseudo_docs.append(np.random.choice(len(document_distribution), size=doc_length, p=document_distribution)) \n",
    "            pseudo_labels.append(generatePseudoLabels(alpha, word_distributions, topic))\n",
    "        \n",
    "        topic_docs[topic] = (pseudo_docs, pseudo_labels)\n",
    "        \n",
    "    return topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_docs = generateLabelledPseudoDocuments(alpha = 0.3, doc_length = 1000, num_docs = 1000)\n",
    "#pseudo_docs = np.load('pseudo_docs.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generateTrainingData(pseudo_docs, word_embedding):\n",
    "#     \"\"\" Transform the index of corresponding word into its embeddings\n",
    "#         Input: pseudo_docs - pseudo_docs dictionary\n",
    "#                word_embedding - word embedding matrix \n",
    "#         Output: a stack of word vectors of all words in pseudo docs\n",
    "    \n",
    "#     \"\"\"\n",
    "#     training_data  = {}\n",
    "#     for topic in pseudo_docs.keys():\n",
    "#         pseudo_docs_wv = list(map(lambda x: word_embedding[x, :], pseudo_docs[topic][0]))\n",
    "#         #pseudo_labels = \n",
    "#         training_data[topic] = (pseudo_docs_wv, pseudo_docs[topic][1])\n",
    "#     return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = generateTrainingData(pseudo_docs, word_embedding)\n",
    "# # np.save('training_data.npy', dict(training_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
