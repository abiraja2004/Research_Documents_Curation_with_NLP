{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Prediction Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "from gensim.parsing.preprocessing import strip_short,strip_punctuation,\\\n",
    "                                         strip_numeric, strip_multiple_whitespaces\n",
    "\n",
    "import glob\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_to_index(file_path):\n",
    "    articles = []\n",
    "    labels = []\n",
    "    for i in glob.glob(file_path + '/*.txt'):\n",
    "        try:\n",
    "            paper = open(i, encoding='utf-8')\n",
    "            articles.append(paper.read())\n",
    "            labels.append(i.split('/')[-1].split('.')[0][5:])\n",
    "        except:\n",
    "            pass\n",
    "    # Clear out newline characters and non-unicode characters, concatenate words separated with '- '\n",
    "    cleaned_articles = list(map(lambda x:x.lower(), articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r\"[^a-zA-Z0-9()_-]\", ' ', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r\"- \", \"\", x), cleaned_articles))\n",
    "    # strip contents between brackets\n",
    "    cleaned_articles = list(map(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x), cleaned_articles))\n",
    "    # strip the words start with x\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bx.*?\\b', '', x), cleaned_articles))\n",
    "    # strip the words start with y, not followed by a vow\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\by[^aeiou].*?\\b', '', x), cleaned_articles))\n",
    "    # remove words that contains digits\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\w*\\d\\w*\\s*', '', x), cleaned_articles))\n",
    "    # remove 'max', 'min', 'sup', 'lim', 'exp', 'eqz'\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bmax\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bmin\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bsup\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\blim\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\bexp\\b\\s*', '', x), cleaned_articles))\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\beqz\\b\\s*', '', x), cleaned_articles))\n",
    "    # remove consecutive duplicate\n",
    "    cleaned_articles = list(map(lambda x: re.sub(r'\\b(\\w+)\\s+\\1\\b\\s*', '', x), cleaned_articles))\n",
    "    # strip references section\n",
    "    cleaned_articles = list(map(lambda x: ''.join(x.split('reference')[:-1])\n",
    "                                if x.find('reference') != -1 else x, cleaned_articles))\n",
    "    # Strip out characters that are less than 3\n",
    "    def preprocess_text(s):\n",
    "        \"\"\"Remove unwanted text formats with numeric, whitespace, punctuation, short words stripped \n",
    "           Input: text string\n",
    "           Output: post processed string\n",
    "        \"\"\"\n",
    "        s = strip_multiple_whitespaces(s)\n",
    "        s = strip_punctuation(s)\n",
    "        s = strip_short(s, minsize = 3)\n",
    "        regex = re.compile('[^\\w]')\n",
    "        regex.sub('', s)\n",
    "        return s\n",
    "    cleaned_articles = list(map(preprocess_text, cleaned_articles))\n",
    "    cleaned_sentences = []\n",
    "    for i in cleaned_articles:\n",
    "        cleaned_sentences += list(map(lambda x: x, tokenize.sent_tokenize(i)))\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + ['within', 'however']) \n",
    "    # Strip stopwords, tokenize sentence to words\n",
    "    cleaned_sentences_w = list(map(lambda sentence: [w for w in tokenize.word_tokenize(sentence) if not w in stop_words], \n",
    "                              cleaned_sentences))\n",
    "    # bigram transform\n",
    "    bigram_transformer = Phrases(cleaned_sentences_w)\n",
    "    return list(bigram_transformer[cleaned_sentences_w]), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_word_lists, papers_labels = docs_to_index('./extracted_papers/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map it to vocab to get index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"./word vectors.kv\"\n",
    "model = KeyedVectors.load(filename, mmap='r')\n",
    "word_embedding = np.array(model.wv.vectors)\n",
    "vocab = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_word_index_list_with_undefined = list(map(lambda x: list(map(lambda y: vocab.index(y)+1 if y in vocab else -1, x)),\n",
    "                                                 papers_word_lists))\n",
    "papers_word_index_list_cleaned = list(map(lambda x: list(filter(lambda y: y > 0, x))[:5000],\n",
    "                                          papers_word_index_list_with_undefined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_word_index_list_padded = list(map(lambda x: x + [0]*(5000-len(x)), papers_word_index_list_cleaned))\n",
    "papers_word_index_list = np.array(papers_word_index_list_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./word_index_list_test.npy', papers_word_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./labels_test.npy', np.array(papers_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
